---
title: "Summary of the Output produced so far"
author: "Arturo Bertero"
date: "2024-04-15"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, error = FALSE, message = FALSE, warning = FALSE)
```

```{r}
library("pacman")
p_load(tidyverse, here, haven, countrycode, vdemdata, psych, lavaan, ltm,
       janitor, conflicted, sjPlot, BMA, BMS, fastDummies, ggplot2, reshape2,
       hrbrthemes, viridis)

#conflicts
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
```

```{r}
# import final data
data = read_rds(here("Input", "final_data", "final.rds"))

#as factor
data$country = as.factor(data$country)
data$year = as.factor(data$year)
data$country_year = as.factor(data$country_year)

# data preparation for bms 

bms <- data %>%
  # Creating dummy variables for 'country' and 'year'
  dummy_cols(select_columns = c("country", "year"), remove_selected_columns = T) %>% 
  # exclude one dummy for country and one for year
  select(-c(country_AUT, year_1989))

# create data frame with DV as first col
#cor_cons
bms_cor_cons = bms %>% 
  select(EES_COR_constraint, alt_inf:gallagher, country_BEL:year_2019)

#cor_aspl
bms_cor_aspl = bms %>% 
  select(EES_COR_aspl, alt_inf:p_inst, pint, pola, ihdi:gallagher, country_BEL:year_2019)

# glasso_cor
bms_glasso_cons = bms %>% 
  select(EES_GGM_glasso_constraint, alt_inf:p_inst, pint, pola, ihdi:gallagher, country_BEL:year_2019)

# glasso_aspl
bms_glasso_aspl = bms %>% 
  select(EES_GGM_glasso_aspl, alt_inf:p_inst, pint, pola, ihdi:gallagher, country_BEL:year_2019)

# huge_cor
bms_huge_cons = bms %>% 
  select(EES_GGM_huge_constraint, alt_inf:p_inst, pint, pola, ihdi:gallagher, country_BEL:year_2019)

# huge_aspl
bms_huge_aspl = bms %>% 
  select(EES_GGM_huge_aspl, alt_inf:p_inst, pint, pola, ihdi:gallagher, country_BEL:year_2019)
```
# Introduction
In this document I show the most relevant output I have produced so far. In part 1 I focus on the determinants of beliefs' constraint. I show the analytical strategy with which we created our dependent variables and some plot to show their frequency distributions. Moreover, I show regression models explaining constraint and ASPL of PTV networks. In part 2 I investigate the consequences of the connectivity of PTV networks. In particular, I explore the impact network characteristics have on turnout behavior. 

# Research design
For every country and every year i produce three kinds of correlational networks: plain correlation, glasso and huge (partial correlation networks without and with non parametric transformation). On each of them i calculate the following measures: 

1. **Constraint**: mean absoulte values of correlations
2. **ASPL**: average shortest path length of the network
3. **np_i**: Ratio negative positive edges, fraction of integer numbers
4. **np_w**: Ratio negative positive edges, fraction of edge weights
5. **EGA**: dimensionality
6. **CCA**: number of correlational class

## The Italian example
Now I show you the Italian example. Networks are glasso GGM. 

![Italian electoral competition in 1989](/Users/art/Documents/Github/lavori in corso/papers/comp_pbs/Output/untitled folder/net_1989.jpg)
![Italian electoral competition in 1994](/Users/art/Documents/Github/lavori in corso/papers/comp_pbs/Output/untitled folder/net_1994.jpg)

![Italian electoral competition in 1999](/Users/art/Documents/Github/lavori in corso/papers/comp_pbs/Output/untitled folder/net_1999.jpg)

![Italian electoral competition in 2004](/Users/art/Documents/Github/lavori in corso/papers/comp_pbs/Output/untitled folder/net_2004.jpg)

![Italian electoral competition in 2009](/Users/art/Documents/Github/lavori in corso/papers/comp_pbs/Output/untitled folder/net_2009.jpg)

![Italian electoral competition in 2014](/Users/art/Documents/Github/lavori in corso/papers/comp_pbs/Output/untitled folder/net_2014.jpg)

![Italian electoral competition in 2019](/Users/art/Documents/Github/lavori in corso/papers/comp_pbs/Output/untitled folder/net_2019.jpg)

# Descriptives
A basic visualizations of aggregated constraint (over time and country). The application of non-paranormal transformation from the huge package does not make the constraint of glasso and huge networks too different. Regularization shrinks correlation coefficient of the pcor networks, that are centered on smaller values in the violin plots. 

```{r}
# data preparation
violin_data = data %>% 
  select("EES_COR_constraint", "EES_GGM_huge_constraint", "EES_GGM_glasso_constraint") 

long_violin_data = melt(violin_data, variable.name = "Model", value.name = "Value")

# violin plot

# Calculate the sample sizes
sample_size <- long_violin_data %>% group_by(Model) %>% summarize(num=n())

# Join the sample and violin data 
plot_data <- long_violin_data %>%
  left_join(sample_size, by = "Model") %>%
  mutate(myaxis = paste0(Model, "\n", "n=", num))

# Create the plot
violin_plot <- ggplot(plot_data, aes(x=myaxis, y=Value, fill=Model)) +
  geom_violin(position=position_dodge(width=0.25), width=0.8) +
  geom_boxplot(width=0.1, color="grey", alpha=0.2, position=position_dodge(width=0.25)) +
  scale_fill_viridis(discrete = TRUE) +
  theme_ipsum(base_size = 14) +
  theme(
    legend.position = "none",
    plot.title = element_text(size=14)
  ) +
  labs(title="Aggregated distribution of Constraint", x="", y="Constraint") +
  scale_x_discrete(labels=c("Correlation", "Glasso", "Huge"))

# Print 
print(violin_plot)
```

A basic visualizations of aggregated ASPL (over time and country). Once again, the ASPL of the pcor network is comparable. ASPL show more variance than constraint. The tuning parameter of partial correlational models is kept constant, and is found by minimizing the extent of the hyperparameter, preserving a fully connected network. In this way, ASPL is always lower than infinite. 

```{r}
# data preparation
violin_data_aspl = data %>% 
  select("EES_COR_aspl", "EES_GGM_huge_aspl", "EES_GGM_glasso_aspl") 

long_violin_data_aspl = melt(violin_data_aspl, variable.name = "Model", value.name = "Value")

# violin plot

# Calculate the sample sizes
sample_size <- long_violin_data_aspl %>% group_by(Model) %>% summarize(num=n())

# Join the sample and violin data 
plot_data_aspl <- long_violin_data_aspl %>%
  left_join(sample_size, by = "Model") %>%
  mutate(myaxis = paste0(Model, "\n", "n=", num))

# Create the plot
violin_plot_aspl <- ggplot(plot_data_aspl, aes(x=myaxis, y=Value, fill=Model)) +
  geom_violin(position=position_dodge(width=0.25), width=0.8) +
  geom_boxplot(width=0.1, color="grey", alpha=0.2, position=position_dodge(width=0.25)) +
  scale_fill_viridis(discrete = TRUE) +
  theme_ipsum(base_size = 14) +
  theme(
    legend.position = "none",
    plot.title = element_text(size=14)
  ) +
  labs(title="Aggregated distribution of ASPL", x="", y="ASPL") +
  scale_x_discrete(labels=c("Correlation", "Glasso", "Huge"))

# Print 
print(violin_plot_aspl)
```

Constraint by time and country. Model type is visualized with colored lines, years on the x axis, constraint in the y. Despite the obvious gap between regularized and not regularized methods, the measures seems to correlate strongly across countries and waves. The image clutters in the html file. Full resolution file available on GitHub. 

```{r fig.width=16, fig.height=8}
# data
cons_trend <- data %>%
  select(year, country, EES_COR_constraint, EES_GGM_huge_constraint, EES_GGM_glasso_constraint) 

long_cons_trend = melt(cons_trend, variable.name = "Model", value.name = "Value")


#graph
cons_trend_graph = long_cons_trend %>%
  ggplot(aes(x = year, y = Value, group = interaction(country, Model), color = Model)) +
    geom_line() +
    geom_point() +
    facet_wrap(~country) +  
    scale_color_viridis_d(labels = c("Correlation", "Glasso", "Huge")) +  # Custom legend labels
    theme_minimal() +
    theme(legend.position = "bottom") +
    labs(title = "Temporal Development of Constraint",
         x = "Year", 
         y = "Constraint",
         color = "Model Type") 

# Print the graph
print(cons_trend_graph)
```

ASPL by time and country. The correlation between these measures are even stronger than those of constraint. Values are pretty much stationary. 

```{r fig.width=16, fig.height=8}
# data
aspl_trend <- data %>%
  select(year, country, EES_COR_aspl, EES_GGM_huge_aspl, EES_GGM_glasso_aspl) 

long_aspl_trend = melt(aspl_trend, variable.name = "Model", value.name = "Value")


#graph
aspl_trend_graph = long_aspl_trend %>%
  ggplot(aes(x = year, y = Value, group = interaction(country, Model), color = Model)) +
    geom_line() +
    geom_point() +
    facet_wrap(~country) +  
    scale_color_viridis_d(labels = c("Correlation", "Glasso", "Huge")) +  # Custom legend labels
    theme_minimal() +
    theme(legend.position = "bottom") +
    labs(title = "Temporal Development of ASPL",
         x = "Year", 
         y = "Constraint",
         color = "Model Type") 

# Print the graph
print(aspl_trend_graph)
```

# Part 1: What does predict network connectivity? 
In this section I explore the determinants of the connectedness of a PTV network. My literature review highlighted some potential factors that were observed to *predict* high attitudinal constraint for political belief systems: 

1. Party institutionalization (**p_inst**, the Vdem one, since the index of Keskinturk was always discarded by BMA)
2. Party ideological institutionalization (**ideol**)
3. Party polarization (**pola**)
4. Strength of mass partisanship (**mass_mob**)
5. Political interest (**pint**)
6. Education (**educ**)

It is also important to *control* for the following factors: 

1. Effective Number of Electoral Parties (**ENEP**)
2. Gallagher disproportionality index (**gallagher**)
3. Inequality-adjusted Human Development Index (**ihdi**)
4. Alternative source of information index (**alt_inf**)

## Method for regression: BMA
We have many potential predictors, and only 143 observation (country_year units). With this small sample size and data structure we don't meet the requirements for multilevel models (as specified [here](https://econtent.hogrefe.com/doi/10.1027/1614-2241.1.3.86) ). Therefore we decided to use fixed effects for countries and years. Second, since we have many potential predictors, I follow an inductive approach to regression models. I use Bayesian Model Averaging, a statistical technique that involves averaging over models in the Bayesian framework rather than selecting a single model. This method incorporates model uncertainty by weighing each model by its posterior model probability, which is derived from the data and prior beliefs about the models. One of the output of a BMA is a table were predictors are associated with a statistic called Posterior Inclusion Probability (PIP). PIP measures the probability that a given variable is included in the true model based on the data analyzed. PIP values greater than .50 represent strong evidence that the variable in question should be used to capture the variance of a dependent variable. BMA is implemented with the [BMS package](https://cran.r-project.org/web/packages/BMS/index.html). For an introduction on BMA see [here](https://journals.sagepub.com/doi/full/10.1177/2515245919898657?trk=public_post_comment-text), and [here](https://www.tandfonline.com/doi/abs/10.1080/01621459.1997.10473615?casa_token=FRiPi6xu3EcAAAAA:UIUI7bGn6WOyzysdGiwB9cXrdDLow9pf6iSYu8olmHfh69AL6yrM67rW1Lr6dDcIP3a0Yw_qrQoO) for the original paper.

## Determinants of constraint

### Correlational networks
The determinants of constraint are not well identified in the data we have. Let's start with the BMA for the constraint of correlational networks. If we focus on the PIP column, we see *Party Polarization* is the only predictor that has PIP > .50. Country and year variables have PIP = 100 because I specified them as fixed term for the regression. Without this specification, the Bayesian approach would still prompt us to insert many dummy variables for country and year, effectively supporting our initial choice. 

```{r}
model_cor_con <- bms(bms_cor_cons, fixed.reg = c(
"country_BEL","country_BGR", "country_CYP",  "country_CZE","country_DEU",
"country_DNK","country_ESP", "country_EST",  "country_FIN","country_FRA","country_GBR",
"country_GRC", "country_HRV", "country_HUN","country_IRL","country_ITA",
"country_LTU", "country_LUX", "country_LVA","country_MLT","country_NLD",
"country_POL", "country_PRT", "country_ROU","country_SVK","country_SVN",
"country_SWE", "year_1994"  ,"year_1999"  ,"year_2004",
"year_2009", "year_2014","year_2019"))

#estimates.bma(model_cor_con) # pip > 50 for pola
```

On the basis of BMA we can fit an OLS with fixed effects where we have an increasing number of predictors. For all regressions I will produce three nested models, where I gradually insert the predictors that are ranked as most credible by BMA. In this case I will add *ideol* and *pint* (PIP = 0.30 and 0.27). I avoid presenting coefficients for fixed effects. Despite a very high R2, these models do not consent to isolate predictors with confidence. Pola is the only variable with three stars, and only in M1. Coefficients are always really small, and the positive coefficient of pint goes in the opposite of what we excpected. This result will be confirmed in following models. 

```{r}
cor_constraint <- lm(EES_COR_constraint ~ pola + country + year , data = data)
cor_constraint_2 <- lm(EES_COR_constraint ~ pola + ideol + country + year , data = data)
cor_constraint_3 <- lm(EES_COR_constraint ~ pola + ideol + pint + country + year , data = data)

tab_model(list(cor_constraint, cor_constraint_2, cor_constraint_3),
          show.p = TRUE,
          p.style = "stars",
          p.threshold = c(0.1, 0.05, 0.01),
          dv.labels = c("M1", "M2", "M3"),
          collapse.ci = TRUE,
          show.aic = TRUE,
          title = "OLS fixed effects (country and years) on constraint of cor network",
          string.pred = " ",
          auto.label = FALSE,
          terms = c("pola", "ideol", "pint"))
```

### Glasso networks
Here is the BMA for the glasso. Again, no PIP < .50. 
```{r}
model_glasso_con <- bms(bms_glasso_cons, fixed.reg = c(
"country_BEL","country_BGR", "country_CYP",  "country_CZE","country_DEU",
"country_DNK","country_ESP", "country_EST",  "country_FIN","country_FRA","country_GBR",
"country_GRC", "country_HRV", "country_HUN","country_IRL","country_ITA",
"country_LTU", "country_LUX", "country_LVA","country_MLT","country_NLD",
"country_POL", "country_PRT", "country_ROU","country_SVK","country_SVN",
"country_SWE", "year_1994"  ,"year_1999"  ,"year_2004",
"year_2009", "year_2014","year_2019"))

#estimates.bma(model_glasso_con) # no pip > 50 
```

Now I will show models with the first three variables in terms of PIP: *p_inst*, *ENEP*, and *pint*. Only p_inst reaches two stars, and this time with a huge and positive coefficient. 

```{r}
glasso_constraint <- lm(EES_GGM_glasso_constraint ~ p_inst + country + year , data = data)
glasso_constraint_2 <- lm(EES_GGM_glasso_constraint ~ p_inst + ENEP + country + year , data = data)
glasso_constraint_3 <- lm(EES_GGM_glasso_constraint ~ p_inst + ENEP + pint + country + year , data = data)

tab_model(list(glasso_constraint, glasso_constraint_2, glasso_constraint_3),
          show.p = TRUE,
          p.style = "stars",
          p.threshold = c(0.1, 0.05, 0.01),
          dv.labels = c("M1", "M2", "M3"),
          collapse.ci = TRUE,
          show.aic = TRUE,
          title = "OLS fixed effects (country and years) on constraint of glasso network",
          string.pred = " ",
          auto.label = FALSE,
          terms = c("p_inst", "ENEP", "pint"))
```

### Huge networks
Here is the BMA for the constraint of Huge. Again, no variable reaches the treshold of .50.
```{r}
model_huge_con <- bms(bms_huge_cons, fixed.reg = c(
"country_BEL","country_BGR", "country_CYP",  "country_CZE","country_DEU",
"country_DNK","country_ESP", "country_EST",  "country_FIN","country_FRA","country_GBR",
"country_GRC", "country_HRV", "country_HUN","country_IRL","country_ITA",
"country_LTU", "country_LUX", "country_LVA","country_MLT","country_NLD",
"country_POL", "country_PRT", "country_ROU","country_SVK","country_SVN",
"country_SWE", "year_1994"  ,"year_1999"  ,"year_2004",
"year_2009", "year_2014","year_2019"))

#estimates.bma(model_huge_con) # no pip > 50 
```
Now I build a model with the three chosen variables, *p_inst*, *pint*, *pola*. P_inst is significant and has a huge coefficient. Pint and Pola seems irrelevant. 

```{r}
huge_constraint <- lm(EES_GGM_huge_constraint ~ p_inst + country + year , data = data)
huge_constraint_2 <- lm(EES_GGM_huge_constraint ~ p_inst + pint + country + year , data = data)
huge_constraint_3 <- lm(EES_GGM_huge_constraint ~ p_inst + pint + pola + country + year , data = data)

tab_model(list(huge_constraint, huge_constraint_2, huge_constraint_3),
          show.p = TRUE,
          p.style = "stars",
          p.threshold = c(0.1, 0.05, 0.01),
          dv.labels = c("M1", "M2", "M3"),
          collapse.ci = TRUE,
          show.aic = TRUE,
          title = "OLS fixed effects (country and years) on constraint of huge network",
          string.pred = " ",
          auto.label = FALSE,
          terms = c("p_inst", "pint", "pola"))
```

## Determinant of ASPL
ASPL has two advantages over constraint. First, it has inherently more variance, since it is a measure that has as a theoretical limit of + infinite. Indeed, when even a single node is disconnected, the shortest path to that node are per definition infinite (no amount of steps which can brings you to node A to disconnected node B). Second, unlike constraint, ASPL is part of the toolbox of network science. This means that this metric, unlike constraint, makes sense only when variables are analyzed in a networked fashion. This is not true for constraint, which could be obtained without any commitment to a network approach to survey variable. Unlike constraint, which does not consider the network-location of a correlation, ASPL punishes network that has lowly connected nodes, as it gives differnt weights to different correlation pairs, depending on their topological position in the network. 

In our data, the determinants of ASPL are much more clearer than those of constraint. 

### Correlational networks
Here is the BMA on the ASPL of correlational networks: *pola*, *pint*, *p_inst* seems the most relevant predictors, although none of them reaches 0.50. 
```{r}
model_cor_aspl <- bms(bms_cor_aspl, fixed.reg = c(
"country_BEL","country_BGR", "country_CYP",  "country_CZE","country_DEU",
"country_DNK","country_ESP", "country_EST",  "country_FIN","country_FRA","country_GBR",
"country_GRC", "country_HRV", "country_HUN","country_IRL","country_ITA",
"country_LTU", "country_LUX", "country_LVA","country_MLT","country_NLD",
"country_POL", "country_PRT", "country_ROU","country_SVK","country_SVN",
"country_SWE", "year_1994"  ,"year_1999"  ,"year_2004",
"year_2009", "year_2014","year_2019"))

#estimates.bma(model_cor_aspl) # no pip > 50 
```

Here is the model with these three predictors. Pola reaches 3 stars even in M3, with a negative coefficient (= more connectivity). P_inst is marginally significant, with negative coefficient. Pint is once again in the opposite direction: it has a positive coefficient, which is marginal. 
```{r}
cor_aspl <- lm(EES_COR_aspl ~ pola + country + year , data = data)
cor_aspl_2 <- lm(EES_COR_aspl ~ pola + pint + country + year , data = data)
cor_aspl_3 <- lm(EES_COR_aspl ~ pola + pint + p_inst + country + year , data = data)

tab_model(list(cor_aspl, cor_aspl_2, cor_aspl_3),
          show.p = TRUE,
          p.style = "stars",
          p.threshold = c(0.1, 0.05, 0.01),
          dv.labels = c("M1", "M2", "M3"),
          collapse.ci = TRUE,
          show.aic = TRUE,
          title = "OLS fixed effects (country and years) on ASPL of correlational network",
          string.pred = " ",
          auto.label = FALSE,
          terms = c("pola", "pint", "p_inst"))
```

### Glasso networks
Here is the BMA for the ASPL of glasso networks. The picture is very clear: *pola*, *p_inst*, *pint* play a fundamental role in the model. Also the index *IHDI* has PIP > 0.50. 

```{r}
model_glasso_aspl <- bms(bms_glasso_aspl, fixed.reg = c(
"country_BEL","country_BGR", "country_CYP",  "country_CZE","country_DEU",
"country_DNK","country_ESP", "country_EST",  "country_FIN","country_FRA","country_GBR",
"country_GRC", "country_HRV", "country_HUN","country_IRL","country_ITA",
"country_LTU", "country_LUX", "country_LVA","country_MLT","country_NLD",
"country_POL", "country_PRT", "country_ROU","country_SVK","country_SVN",
"country_SWE", "year_1994"  ,"year_1999"  ,"year_2004",
"year_2009", "year_2014","year_2019"))

#estimates.bma(model_glasso_aspl) 
```
Here the nested model with this variables. Only IHDI has two stars, all other estimates are reliable (3 stars even in M4), and their coefficients point in the expected direction. Moreover, the coefficient of p_inst is very huge, as in Keskinturk.
```{r}
glasso_aspl <-   lm(EES_GGM_glasso_aspl ~ pola + country + year , data = data)
glasso_aspl_2 <- lm(EES_GGM_glasso_aspl ~ pola + p_inst + country + year , data = data)
glasso_aspl_3 <- lm(EES_GGM_glasso_aspl ~ pola + p_inst + pint + country + year , data = data)
glasso_aspl_4 <- lm(EES_GGM_glasso_aspl ~ pola + p_inst + pint + ihdi + country + year , data = data)

tab_model(list(glasso_aspl, glasso_aspl_2, glasso_aspl_3, glasso_aspl_4),
          show.p = TRUE,
          p.style = "stars",
          p.threshold = c(0.1, 0.05, 0.01),
          dv.labels = c("M1", "M2", "M3", "M4"),
          collapse.ci = TRUE,
          show.aic = TRUE,
          title = "OLS fixed effects (country and years) on ASPL of glasso network",
          string.pred = " ",
          auto.label = FALSE,
          terms = c("pola", "p_inst", "pint", "ihdi"))
```

### Huge networks
Here is the BMA for the ASPL of Huge networks. *Pola*, *p_inst* and *pint* have PIP greater than 0.50. 

```{r}
model_huge_aspl <- bms(bms_huge_aspl, fixed.reg = c(
"country_BEL","country_BGR", "country_CYP",  "country_CZE","country_DEU",
"country_DNK","country_ESP", "country_EST",  "country_FIN","country_FRA","country_GBR",
"country_GRC", "country_HRV", "country_HUN","country_IRL","country_ITA",
"country_LTU", "country_LUX", "country_LVA","country_MLT","country_NLD",
"country_POL", "country_PRT", "country_ROU","country_SVK","country_SVN",
"country_SWE", "year_1994"  ,"year_1999"  ,"year_2004",
"year_2009", "year_2014","year_2019"))

#estimates.bma(model_huge_aspl) #pip > 50 pola p_inst
```

Here are nested models. Once again very big and significant coefficients for pola and p_inst. Pint, although pointing in the expected direction, only has one star. 
```{r}
huge_aspl <-   lm(EES_GGM_huge_aspl ~ pola + country + year , data = data)
huge_aspl_2 <- lm(EES_GGM_huge_aspl ~ pola + p_inst + country + year , data = data)
huge_aspl_3 <- lm(EES_GGM_huge_aspl ~ pola + p_inst + pint + country + year , data = data)

tab_model(list(huge_aspl, huge_aspl_2, huge_aspl_3),
          show.p = TRUE,
          p.style = "stars",
          p.threshold = c(0.1, 0.05, 0.01),
          dv.labels = c("M1", "M2", "M3"),
          collapse.ci = TRUE,
          show.aic = TRUE,
          title = "OLS fixed effects (country and years) on ASPL of huge network",
          string.pred = " ",
          auto.label = FALSE,
          terms = c("pola", "p_inst", "pint"))
```

### Summary: which are the main predictors?
So, what does predict the connectivity of these behavioral intentions? In this table I show the most important predictors of constraint and ASPL, according to BMA. 

|  **Model/Measure**  	| **Predictor 1** 	| **Predictor 2** 	| **Predictor 3** 	|
|:-------------------:	|:---------------:	|:---------------:	|:---------------:	|
|   _Cor Constraint_  	|       pola      	|      ideol      	|       pint      	|
| _Glasso Constraint_ 	|      p_inst     	|       ENEP      	|       pint      	|
|  _Huge Constraint_  	|      p_inst     	|       pint      	|       pola      	|
|      _Cor ASPL_     	|       pola      	|       pint      	|      p_inst     	|
|    _Glasso ASPL_    	|       pola      	|      p_inst     	|       pint      	|
|     _Huge ASPL_     	|       pola      	|      p_inst     	|       pint      	|

Essentially, regardless of the type of measure and model, we have evidence that only a bunch of variables play an important role. Party polarization (**pola**) predict constraint and ASPL in 5 cases out of 5. Party institutionalization (**p_inst**) predict constraint and ASPL in 5 cases as well. Political interest (**pint**) is involved in all cases. 

Now I fit a regression model in which each dependent variable (Every possible combination of method and measure: 6 variables) is explained in a model where I insert the three variables pola, p_inst, and pint. As control I use fixed effects for country and year, ENEP, gallagher and IHDI. I interpret these models in the following way: 

1. Reconfirmation of the fact we are better at predicting aspl than constraint
2. Party institutionalization seems the best predictor across measures and method. However, p_inst does not predict any measure of a cor network.
3. The role of pint is chaotic: sometimes it leads to higher connectivity (in pcor methods), sometimes it lowers it (cor). Stratifying the sample for this variable and perform a multilevel model would probably help, at the cost of having networks estimated on a fraction of the sample. 

```{r}
th_1 = lm(EES_COR_constraint ~ pola + p_inst + pint + country + year + ENEP + gallagher + ihdi, data = data)
th_2 = lm(EES_GGM_glasso_constraint ~ pola + p_inst + pint + country + year + ENEP + gallagher + ihdi, data = data)
th_3 = lm(EES_GGM_huge_constraint ~ pola + p_inst + pint + country + year + ENEP + gallagher + ihdi, data = data)
th_4 = lm(EES_COR_aspl ~ pola + p_inst + pint + country + year + ENEP + gallagher + ihdi, data = data)
th_5 = lm(EES_GGM_glasso_aspl ~ pola + p_inst + pint + country + year + ENEP + gallagher + ihdi, data = data)
th_6 = lm(EES_GGM_huge_aspl ~ pola + p_inst + pint + country + year + ENEP + gallagher + ihdi, data = data)

tab_model(list(th_1, th_2, th_3, th_4, th_5, th_6),
          show.p = TRUE,
          p.style = "stars",
          p.threshold = c(0.1, 0.05, 0.01),
          dv.labels = c("Cor_const", "glasso_const", "huge_const",
                        "Cor_aspl", "glasso_aspl", "huge_aspl"),
          collapse.ci = TRUE,
          show.aic = TRUE,
          title = "OLS fixed effects (country and years) common model for all DV",
          string.pred = " ",
          auto.label = FALSE,
          terms = c("pola", "p_inst", "pint"))
```

# Part 2: What are the consequences of network characteristics? 
In the first part of the report we have found that the characteristic of the networks of political behavior intentions surveyed in 28 EU countries between 1989 and 2019 depend on the structure of national political fields. The levels of party institutionalization, party polarization, the magnitude with which party use ideology in their platforms, and country-level political interest leads to more connected PTV networks. But what are the consequences of an highly connected network?

I thinks two hypotheses must be tested. They vaguely descend from the belief system theory, and I test them on turnout at the EU election.  

1. The more PTV networks are connected at the national level, the higher the turnout. If people perceive the electoral competition in an organized and coherent way, they will be more likely to vote. An unstructured PTV network is instead pushing voters in the opposite direction: people do not know the competitior of an election, and they do not know their relationship. This can create pressure leading to abstension. 

2. Yet, this is not enough. I think another set of measure that can help us predicting turnout pertain to the ratio negative/positive links in the networks. I think the more people are able to organize political parties recurring to oppositional schemes (e.g.: negative links on top of positive ones) the more they will feel confident in go voting. On the opposite, if people are only able to perceive similarities between national parties, they should be less likely to vote. A natural extension of this second route of investigation would be to assess the similarity between the network measures pertaining to the ratio between negative and positive link with the [well known measures of affactive polarization that can be created trough party identification items](https://www.cambridge.org/core/journals/european-political-science-review/article/affective-polarization-in-europe/36BDBF804365FE0B350E610E9E7C714E). 

## Does constraint predict turnout? 
A [recent article](https://link.springer.com/article/10.1007/s11109-021-09720-y) suggests variables leading to greater turnout can be grouped in three dimensions: institutional predictors, socio-economic predictors, political predictors. Other research suggest EU elections do not have different predictors with respect to national ones [here](https://www.sciencedirect.com/science/article/pii/S0261379401000543?casa_token=njgNX6Xst08AAAAA:EC8chEiwQuB91zhyjws81Q2h9Qz74vxdcxIOGls8-BX9BL6WhnwKg9EZGBMqWYvhaNq-nhGr) and [here](https://journals.sagepub.com/doi/full/10.1177/1065912911421016?casa_token=UbNArStbxPcAAAAA%3AQXBXl7Zdu3h2hHALyPRIXXT4CYBP7QkRNewK3TmNaAvMC1I_E9-uwe65xt79FRtYYTo5LN7EpUM). 

The most important *institutional predictors* are compulsory voting and proportional representation. Three countries in our sample have compulsory voting for the EU elections: Belgium Cyprus, and Luxembourg. I am struggling a bit on finding data on the proportionality of electoral system for EU elections. All countries are forced to adopt a proportional type, but nuances do exist. At the moment I will only include **compulsory voting** in the analyses. 

The most solid *socio-economic* predictors are GDP, ihdi, and inflation. I import **GDP** and **inflation** data. 

Finally, *political predictors* include **party polarization**, **previous turnout shares** and **ENEP**. For the moment I won't add the lagged turnout shares to the models.  

### Does connectivity predict greater turnout?

In short, no, it doesn't. Here are some models where we can see that the effect of connectivity is not significant, if considering mandatory voting. These are baseline models where turnout = mand (mandatory voting) + aspl/constraint of each network type. I also have insterted fixed effects for country and year. 

```{r}
# import final data
turnout = read_rds(here("Input", "final_data", "final_turnout.rds"))

#as factor
turnout$country = as.factor(turnout$country)
turnout$year = as.factor(turnout$year)
turnout$country_year = as.factor(turnout$country_year)
turnout$infla = as.numeric(turnout$infla)

# reg turnout
tur_1 = lm(turnout ~ mand + EES_COR_constraint + country + year, data = turnout)
tur_2 = lm(turnout ~ mand + EES_GGM_glasso_constraint + country + year, data = turnout)
tur_3 = lm(turnout ~ mand + EES_GGM_huge_constraint + country + year, data = turnout)
tur_4 = lm(turnout ~ mand + EES_COR_aspl + country + year, data = turnout)
tur_5 = lm(turnout ~ mand + EES_GGM_glasso_aspl + country + year, data = turnout)
tur_6 = lm(turnout ~ mand + EES_GGM_huge_aspl + country + year, data = turnout)

tab_model(list(tur_1, tur_2, tur_3, tur_4, tur_5, tur_6),
          show.p = TRUE,
          p.style = "stars",
          p.threshold = c(0.1, 0.05, 0.01),
          dv.labels = c("Cor_const", "glasso_const", "huge_const",
                        "Cor_aspl", "glasso_aspl", "huge_aspl"),
          collapse.ci = TRUE,
          show.aic = TRUE,
          title = "OLS fixed effects (country and years) common model for all DV",
          string.pred = " ",
          auto.label = FALSE,
          terms = c("mand", "EES_COR_constraint", "EES_GGM_glasso_constraint", "EES_GGM_huge_constraint",
                    "EES_COR_aspl", "EES_GGM_glasso_aspl", "EES_GGM_huge_aspl"))
```

### Do the ratio measures predict greater turnout?
To test the second research hypothesis I implement another set of regression models. They are once again very basic: turnout = mand + integer ratio/continous ratio of negative/positive links, measured on all network types. Once again we have fixed effects for country and year. Results are somehow interesting: three out of six measures are significant at 0.01 levels. 

```{r}
# reg turnout
tur_sign_1 = lm(turnout ~ mand + EES_COR_np_i + country + year, data = turnout)
tur_sign_2 = lm(turnout ~ mand + EES_COR_np_w + country + year, data = turnout)
tur_sign_3 = lm(turnout ~ mand + EES_GGM_glasso_np_i + country + year, data = turnout)
tur_sign_4 = lm(turnout ~ mand + EES_GGM_glasso_np_w + country + year, data = turnout)
tur_sign_5 = lm(turnout ~ mand + EES_GGM_huge_np_i + country + year, data = turnout)
tur_sign_6 = lm(turnout ~ mand + EES_GGM_huge_np_w + country + year, data = turnout)

tab_model(list(tur_sign_1, tur_sign_2, tur_sign_3, tur_sign_4, tur_sign_5, tur_sign_6),
          show.p = TRUE,
          p.style = "stars",
          p.threshold = c(0.1, 0.05, 0.01),
          dv.labels = c("Cor_const", "glasso_const", "huge_const",
                        "Cor_aspl", "glasso_aspl", "huge_aspl"),
          collapse.ci = TRUE,
          show.aic = TRUE,
          title = "OLS fixed effects (country and years) common model for all DV",
          string.pred = " ",
          auto.label = FALSE,
          terms = c("mand", "EES_COR_np_i", "EES_COR_np_w", "EES_GGM_glasso_np_i",
                    "EES_GGM_glasso_np_w", "EES_GGM_huge_np_i", "EES_GGM_huge_np_w"))



```

Now I build more detailed models. Specifically, I add the control variables introduced above: **GDP**, **inflation**, **ihdi**, **pola** and **ENEP**. Once again I build a set of regression models where the only difference is in the variable measuring the ratio between negative and positive links: 6 variables (3 network types * 2 measures). I only show coefficients for the predictor of interest. To me this is incredible: results remain significants even with (admittedly, too many) control variables. Also, R2 values are out of this world. Only the huge values are critical.

```{r}
tur_sign_1_2 = lm(turnout ~ mand + EES_COR_np_i + gdp + infla + ihdi + pola + ENEP + country + year, data = turnout)
tur_sign_2_2 = lm(turnout ~ mand + EES_COR_np_w + gdp + infla + ihdi + pola + ENEP + country + year, data = turnout)
tur_sign_3_2 = lm(turnout ~ mand + EES_GGM_glasso_np_i + gdp + infla + ihdi + pola + ENEP + country + year, data = turnout)
tur_sign_4_2 = lm(turnout ~ mand + EES_GGM_glasso_np_w + gdp + infla + ihdi + pola + ENEP + country + year, data = turnout)
tur_sign_5_2 = lm(turnout ~ mand + EES_GGM_huge_np_i + gdp + infla + ihdi + pola + ENEP + country + year, data = turnout)
tur_sign_6_2 = lm(turnout ~ mand + EES_GGM_huge_np_w + gdp + infla + ihdi + pola + ENEP + country + year, data = turnout)

tab_model(list(tur_sign_1_2, tur_sign_2_2, tur_sign_3_2, tur_sign_4_2, tur_sign_5_2, tur_sign_6_2),
          show.p = TRUE,
          p.style = "stars",
          p.threshold = c(0.1, 0.05, 0.01),
          dv.labels = c("Cor_const", "glasso_const", "huge_const",
                        "Cor_aspl", "glasso_aspl", "huge_aspl"),
          collapse.ci = TRUE,
          show.aic = TRUE,
          title = "OLS fixed effects (country and years) common model for all DV",
          string.pred = " ",
          auto.label = FALSE,
          terms = c("mand", "EES_COR_np_i", "EES_COR_np_w", "EES_GGM_glasso_np_i",
                    "EES_GGM_glasso_np_w", "EES_GGM_huge_np_i", "EES_GGM_huge_np_w"))
```

# Future directions
I think two things would still worth an investigation. 

1. Assess the **individual level** relationships between measures of connectivity / ratio neg_pos -> voting not voting at the individual level. Panel PTV data would be so nice. 
2. Use **CCA** to build an **inductive typology** of PTVs. The issue here is that network have not the same size. But it would be nice to classify potential voters based on their schema of answers, and than check for which class behavioral intentions are more likely to be finalized in concrete voting behavior. The idea of the inductive typology come from [here](https://www.sciencedirect.com/science/article/abs/pii/S037887331530099X). 

Mark, thanks for this journey! I had a lot of fun and I think at the end we developed something cool. 